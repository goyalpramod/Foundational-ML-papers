{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11538cb-1789-4c9c-a9b2-f1657f2459ae",
   "metadata": {},
   "source": [
    "This is a re-implementation of the original transformers. This is for my general practice, for more of a tutorial structure. Consider going through my previous implementation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b0a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bf925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        assert d_model%num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_K = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_V = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_O = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(Q,K,V, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, num_heads, seq_len_q, d_q)\n",
    "            key: (batch_size, num_heads, seq_len_k, d_k)\n",
    "            value: (batch_size, num_heads, seq_len_v, d_v)\n",
    "            mask: Optional mask to prevent attention to certain positions\n",
    "        \"\"\"\n",
    "        assert Q.shape[-1] == K.shape[-1] #query and key dimension should be equal\n",
    "\n",
    "        attention_score  = (torch.matmul(Q,K.transpose(-2,-1)))/torch.sqrt(torch.tensor(K.shape[-1]))\n",
    "    \n",
    "        # if mask:\n",
    "        #     upper_mask = torch.tril(torch.ones(attention_score.shape[-2], attention_score.shape[-1]))\n",
    "        #     upper_mask.masked_fill_(upper_mask==0, float('-inf'))\n",
    "        #     attention_score = attention_score + upper_mask\n",
    "\n",
    "        # Your mask logic:\n",
    "        # upper_mask = torch.tril(torch.ones(3, 3))  # Lower triangular\n",
    "        # print(\"tril result:\")\n",
    "        # print(upper_mask)\n",
    "        # # Output:\n",
    "        # # [[1, 0, 0],\n",
    "        # #  [1, 1, 0], \n",
    "        # #  [1, 1, 1]]\n",
    "\n",
    "        # upper_mask.masked_fill_(upper_mask==0, float('-inf'))\n",
    "        # print(\"After masked_fill:\")\n",
    "        # print(upper_mask)\n",
    "        # # Output:\n",
    "        # # [[1, -inf, -inf],\n",
    "        # #  [1, 1, -inf],\n",
    "        # #  [1, 1, 1]]\n",
    "\n",
    "        # # Then you ADD this to attention scores:\n",
    "        # attention_score = attention_score + upper_mask\n",
    "\n",
    "        # The above has a problem\n",
    "        # The problems:\n",
    "        # You're adding 1's to allowed positions (should add 0)\n",
    "        # You're adding -inf to masked positions (this is correct)\n",
    "\n",
    "        # FIX\n",
    "        # if mask:\n",
    "        #     mask_matrix = torch.tril(torch.ones(attention_score.shape[-2], attention_score.shape[-1]))\n",
    "        #     mask_matrix = mask_matrix.masked_fill(mask_matrix == 0, float('-inf'))\n",
    "        #     mask_matrix = mask_matrix.masked_fill(mask_matrix == 1, 0.0)  # Don't change allowed positions\n",
    "        #     attention_score = attention_score + mask_matrix\n",
    "\n",
    "        # Simpler fix \n",
    "        if mask:\n",
    "            mask_matrix = torch.triu(torch.ones(attention_score.shape[-2], attention_score.shape[-1]), diagonal=1)\n",
    "            attention_score = attention_score.masked_fill(mask_matrix == 1, float('-inf'))\n",
    "\n",
    "        attention_weights  = F.softmax(attention_score, dim=-1)\n",
    "        assert attention_weights.shape == (Q.shape[0], Q.shape[1], Q.shape[2], K.shape[2])\n",
    "\n",
    "        Z = torch.einsum('bhqk,bhkd -> bhqd', attention_weights, V)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    # def forward(self, input_matrix_1, input_matrix_2, input_matrix_3, mask = 0):\n",
    "    #     batch_size, seq_len = input_matrix_1.shape[0], input_matrix_1.shape[1]\n",
    "    #     # The above is a bug \n",
    "    #     # The problem: You're using input_matrix_1.shape[1] for seq_len, but in the decoder, input_matrix_1, input_matrix_2, and input_matrix_3 might have different sequence lengths!\n",
    "\n",
    "    #     self.Q = self.W_Q(input_matrix_1).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "    #     self.K = self.W_K(input_matrix_2).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "    #     self.V = self.W_V(input_matrix_3).reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "    #     attention_score = self.scaled_dot_product_attention(self.Q, self.K, self.V, mask=mask).transpose(1,2).reshape(batch_size, seq_len,self.d_model)\n",
    "    #     return self.W_O(attention_score)\n",
    "\n",
    "    def forward(self, input_matrix_1, input_matrix_2, input_matrix_3, mask = 0):\n",
    "        batch_size = input_matrix_1.shape[0]\n",
    "\n",
    "        # Get sequence length from each matrix individually\n",
    "        seq_len_q = input_matrix_1.shape[1]  # Query sequence length\n",
    "        seq_len_k = input_matrix_2.shape[1]  # Key sequence length  \n",
    "        seq_len_v = input_matrix_3.shape[1]  # Value sequence length\n",
    "\n",
    "        self.Q = self.W_Q(input_matrix_1).reshape(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1,2)\n",
    "        self.K = self.W_K(input_matrix_2).reshape(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1,2)\n",
    "        self.V = self.W_V(input_matrix_3).reshape(batch_size, seq_len_v, self.num_heads, self.d_k).transpose(1,2)\n",
    "\n",
    "        attention_score = self.scaled_dot_product_attention(self.Q, self.K, self.V, mask=mask)\n",
    "        attention_score = attention_score.transpose(1,2).reshape(batch_size, seq_len_q, self.d_model)\n",
    "\n",
    "        return self.W_O(attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368436ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q, K, V = 0\n",
    "\n",
    "\n",
    "# Z = torch.einsum('bhqk,bhkd -> bhqd', F.softmax((torch.matmul(Q,K.transpose(-2,-1)))/torch.sqrt(torch.tensor(K.shape[-1])), dim=-1), V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67b6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_attention():\n",
    "#     # Small test case\n",
    "#     batch_size, num_heads, seq_len, d_k = 2, 4, 6, 8\n",
    "#     d_model = num_heads * d_k  # 32\n",
    "    \n",
    "#     print(\"=== Testing Scaled Dot Product Attention ===\")\n",
    "    \n",
    "#     # Create sample tensors for scaled_dot_product_attention\n",
    "#     Q = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "#     K = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "#     V = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "    \n",
    "#     # Test without mask\n",
    "#     output = MultiHeadAttention.scaled_dot_product_attention(Q, K, V, mask=False)\n",
    "#     print(f\"SDPA Output shape: {output.shape}\")\n",
    "#     print(f\"SDPA Expected: {(batch_size, num_heads, seq_len, d_k)}\")\n",
    "    \n",
    "#     # Test with mask\n",
    "#     output_masked = MultiHeadAttention.scaled_dot_product_attention(Q, K, V, mask=True)\n",
    "#     print(f\"SDPA Masked output shape: {output_masked.shape}\")\n",
    "    \n",
    "#     print(\"✅ SDPA tests passed!\")\n",
    "    \n",
    "#     print(\"\\n=== Testing Full MultiHeadAttention ===\")\n",
    "    \n",
    "#     # Create MultiHeadAttention module\n",
    "#     mha = MultiHeadAttention(num_heads=num_heads, d_model=d_model)\n",
    "    \n",
    "#     # Create input tensor (batch_size, seq_len, d_model)\n",
    "#     input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
    "#     print(f\"Input shape: {input_tensor.shape}\")\n",
    "    \n",
    "#     # Test forward pass\n",
    "#     try:\n",
    "#         mha_output = mha.forward(input_tensor)\n",
    "#         print(f\"MHA Output shape: {mha_output.shape}\")\n",
    "#         print(f\"MHA Expected: {(batch_size, seq_len, d_model)}\")\n",
    "        \n",
    "#         # Check if output has reasonable values (not NaN or inf)\n",
    "#         if torch.isnan(mha_output).any():\n",
    "#             print(\"❌ Output contains NaN values!\")\n",
    "#         elif torch.isinf(mha_output).any():\n",
    "#             print(\"❌ Output contains infinite values!\")\n",
    "#         else:\n",
    "#             print(\"✅ Output values look reasonable!\")\n",
    "            \n",
    "#         print(\"✅ Full MHA test passed!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ MHA test failed with error: {e}\")\n",
    "#         print(\"Check your forward() method implementation\")\n",
    "    \n",
    "#     print(\"\\n=== Testing with different input sizes ===\")\n",
    "    \n",
    "#     # Test with different sequence length\n",
    "#     seq_len_2 = 10\n",
    "#     input_tensor_2 = torch.randn(batch_size, seq_len_2, d_model)\n",
    "    \n",
    "#     try:\n",
    "#         mha_output_2 = mha.forward(input_tensor_2)\n",
    "#         print(f\"Different seq_len input: {input_tensor_2.shape}\")\n",
    "#         print(f\"Different seq_len output: {mha_output_2.shape}\")\n",
    "#         print(\"✅ Variable sequence length test passed!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Variable sequence length test failed: {e}\")\n",
    "\n",
    "# # Run the test\n",
    "# test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fedea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.ones([seq_len,d_model])\n",
    "        pos = torch.arange(seq_len).unsqueeze(1)\n",
    "        dim = torch.arange(d_model).unsqueeze(0) # More efficient will be to have d_model//2 right here\n",
    "\n",
    "        # self.pe[:, ::2] = torch.sin(self.pos/10000**(2*self.dim[:d_model//2]/d_model)) # This slices about row, not column, So I still have the full tensor\n",
    "        # self.pe[:, 1::2] = torch.cos(self.pos/10000**(2*self.dim[:d_model//2]/d_model))\n",
    "        \n",
    "        pe[:, ::2] = torch.sin(pos/10000**(2*dim[:, :d_model//2]/d_model)) # This slices it in half, this is inefficient tho. Takes more space\n",
    "        pe[:, 1::2] = torch.cos(pos/10000**(2*dim[:, :d_model//2]/d_model))\n",
    "\n",
    "        # Register as buffer - won't be updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        # return input_matrix + self.pe # This has a problem, Our inputs have variable length. It will always give fixed length pe \n",
    "        seq_len = input_matrix.size(1)\n",
    "        return input_matrix + self.pe[:seq_len, :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d40e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the fix\n",
    "# pe = PositionalEncoding(d_model=4, seq_len=10)\n",
    "# test_input = torch.randn(2, 5, 4)\n",
    "# output = pe(test_input)\n",
    "# print(\"Success! Output shape:\", output.shape)\n",
    "# print(\"PE buffer accessible:\", hasattr(pe, 'pe'))\n",
    "# print(\"PE buffer shape:\", pe.pe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6db28d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test Positional Encoding \n",
    "\n",
    "# #\n",
    "# def pe_matrix(d_model, seq_len):\n",
    "\n",
    "#     pe = torch.ones([seq_len,d_model])\n",
    "#     pos = torch.arange(seq_len).unsqueeze(1)\n",
    "#     dim = torch.arange(d_model).unsqueeze(0)\n",
    "\n",
    "#     pe[:, ::2] = torch.sin(pos/10000**(2*dim[:, :d_model//2]/d_model))\n",
    "#     pe[:, 1::2] = torch.cos(pos/10000**(2*dim[:, :d_model//2]/d_model))\n",
    "\n",
    "#     return pe\n",
    "\n",
    "# # Clearer approach\n",
    "# # def pe_matrix(d_model, seq_len):\n",
    "# #     pe = torch.zeros([seq_len, d_model])  \n",
    "# #     pos = torch.arange(seq_len).unsqueeze(1)\n",
    "# #     dim = torch.arange(d_model//2).unsqueeze(0)  \n",
    "    \n",
    "    \n",
    "# #     angles = pos / 10000**(2*dim/d_model)\n",
    "    \n",
    "# #     pe[:, ::2] = torch.sin(angles)\n",
    "# #     pe[:, 1::2] = torch.cos(angles)\n",
    "    \n",
    "# #     return pe\n",
    "\n",
    "# def test_pe_matrix(d_model, seq_len):\n",
    "#     print(f\"Testing PE with d_model={d_model}, seq_len={seq_len}\")\n",
    "    \n",
    "#     # Generate PE matrix\n",
    "#     pe = pe_matrix(d_model, seq_len)\n",
    "    \n",
    "#     # Test 1: Shape check\n",
    "#     print(f\"Shape: {pe.shape} (expected: ({seq_len}, {d_model}))\")\n",
    "#     assert pe.shape == (seq_len, d_model), f\"Shape mismatch!\"\n",
    "    \n",
    "#     # Test 2: Print the matrix to visually inspect\n",
    "#     print(\"PE Matrix:\")\n",
    "#     print(pe)\n",
    "    \n",
    "#     # Test 3: Manual verification for position 0, dimensions 0 and 1\n",
    "#     pos_0 = 0\n",
    "#     expected_dim_0 = torch.sin(torch.tensor(pos_0 / 10000**(2*0/d_model)))  # i=0, so 2i=0\n",
    "#     expected_dim_1 = torch.cos(torch.tensor(pos_0 / 10000**(2*0/d_model)))  # same i=0\n",
    "    \n",
    "#     print(f\"\\nManual check for position 0:\")\n",
    "#     print(f\"PE[0,0] = {pe[0,0]:.6f}, expected = {expected_dim_0:.6f}\")\n",
    "#     print(f\"PE[0,1] = {pe[0,1]:.6f}, expected = {expected_dim_1:.6f}\")\n",
    "    \n",
    "#     # Test 4: Check that values are different across positions\n",
    "#     if seq_len > 1:\n",
    "#         print(f\"\\nDifferent positions check:\")\n",
    "#         print(f\"PE[0,0] = {pe[0,0]:.6f}\")\n",
    "#         print(f\"PE[1,0] = {pe[1,0]:.6f}\")\n",
    "#         print(f\"Different? {not torch.allclose(pe[0,0], pe[1,0])}\")\n",
    "    \n",
    "#     print(\"✓ All tests passed!\")\n",
    "\n",
    "# # Run the test\n",
    "# test_pe_matrix(4, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf13f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,d_model,d_ff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff_model = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,input_matrix):\n",
    "        return self.ff_model(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b36be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,d_model=d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model=d_model,d_ff=d_ff)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.drop_out = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        x = self.mha(input_matrix, input_matrix, input_matrix)\n",
    "        x = self.drop_out(input_matrix + x)\n",
    "        x_1 = self.layer_norm_1(x)\n",
    "        x_1 = self.ffn(x_1)\n",
    "        x_2 = self.drop_out(x + x_1)\n",
    "        x_2 = self.layer_norm_2(x_2)\n",
    "        return x_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb383be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model, d_ff, num_layers, seq_len, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_model = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.pe = PositionalEncoding(d_model=d_model, seq_len=seq_len)\n",
    "        self.encoders = nn.ModuleList([EncoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff) for i in range(num_layers)])\n",
    "        \n",
    "\n",
    "    def forward(self, input_tokens):\n",
    "        input_matrix = self.embedding_model(input_tokens)\n",
    "        input_matrix = self.pe(input_matrix)\n",
    "\n",
    "        for encoder_layer in self.encoders:\n",
    "            input_matrix = encoder_layer(input_matrix)\n",
    "\n",
    "        return input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91d17938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads,d_model=d_model)\n",
    "        self.cross_mha = MultiHeadAttention(num_heads=num_heads,d_model=d_model)\n",
    "        self.ffn = FeedForwardNetwork(d_model=d_model,d_ff=d_ff)\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "        self.drop_out = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_matrix, input_matrix):\n",
    "        x = self.mha(input_matrix, input_matrix, input_matrix, mask=True)\n",
    "        x = self.drop_out(x + input_matrix)\n",
    "        x = self.layer_norm_1(x)\n",
    "        x_1 = self.cross_mha(x, encoder_matrix, encoder_matrix, mask=False)\n",
    "        x_1 = self.drop_out(x_1 + x)\n",
    "        x_1 = self.layer_norm_2(x_1)\n",
    "        x_2 = self.ffn(x_1)\n",
    "        x_2 = self.drop_out(x_2 + x_1)\n",
    "        x_2 = self.layer_norm_3(x_2)\n",
    "        return x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bcd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding_model = nn.Embedding(embedding_dim=d_model, num_embeddings=vocab_size)\n",
    "        self.pe = PositionalEncoding(d_model=d_model,seq_len=seq_len)\n",
    "        self.decoders = nn.ModuleList([DecoderLayer(num_heads=num_heads, d_model=d_model, d_ff=d_ff) for i in range(num_layers)])\n",
    "\n",
    "    def forward(self, output_tokens, encoder_output):\n",
    "        output_matrix = self.embedding_model(output_tokens)\n",
    "        output_matrix = self.pe(output_matrix)\n",
    "\n",
    "        for decoder_layer in self.decoders:\n",
    "            output_matrix = decoder_layer(encoder_output, output_matrix)\n",
    "\n",
    "        return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f075555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, seq_len):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_heads=num_heads, d_model=d_model, d_ff=d_ff, num_layers=num_layers, seq_len=seq_len, vocab_size=src_vocab_size)\n",
    "        self.decoder = Decoder(num_heads=num_heads, d_model=d_model, d_ff=d_ff, num_layers=num_layers, seq_len=seq_len, vocab_size=tgt_vocab_size)\n",
    "        #Vocab sizes may be different\n",
    "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, src_tokens, tgt_tokens):\n",
    "        encoder_output = self.encoder(src_tokens) \n",
    "        \n",
    "        decoder_output = self.decoder(tgt_tokens,encoder_output)\n",
    "        \n",
    "        logits = self.final_layer(decoder_output) #Notice how we do not apply softmax here, as the loss function does that internally\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f58e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vocab(dataset):\n",
    "#     words = dataset.split()\n",
    "#     vocab = {}\n",
    "#     count = 0\n",
    "#     for word in words: \n",
    "#         if word not in vocab.keys():\n",
    "#             vocab[word] = count\n",
    "#             count+=1\n",
    "#     return vocab\n",
    "\n",
    "# def tokenize(sentence, vocab):\n",
    "#     words = sentence.split()\n",
    "#     converted_sentence = []\n",
    "#     for word in words:\n",
    "#         converted_sentence.append(vocab[word])\n",
    "#     return converted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "daffa241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_sentence = \"HI how are you , you look good.\"\n",
    "# print(create_vocab(big_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "003ef8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(dataset, min_freq=1):\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Count word frequencies\n",
    "    # words = [sentence.split() for sentence in dataset] #This has error, it creates a list. Counter doesn't work with lists \n",
    "    \n",
    "    #Fix\n",
    "    # Flatten all sentences into one big list of words\n",
    "    # all_words = []\n",
    "    # for sentence in dataset:\n",
    "    #     all_words.extend(sentence.split()) # Look at the difference between extend and append\n",
    "\n",
    "    # Simpler fix\n",
    "    words = ' '.join(dataset).split()  # Join all sentences, then split\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Create word to index mapping\n",
    "    word_to_idx = {\n",
    "        '<PAD>': 0,\n",
    "        '<UNK>': 1, \n",
    "        '<BOS>': 2,\n",
    "        '<EOS>': 3\n",
    "    }\n",
    "    \n",
    "    # Add words that appear at least min_freq times\n",
    "    idx = 4\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= min_freq:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    # Create reverse mapping - this is the efficient part!\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def tokenize(sentence, word_to_idx, add_special_tokens=True):\n",
    "    words = sentence.split()\n",
    "    tokens = []\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        tokens.append(word_to_idx['<BOS>'])\n",
    "    \n",
    "    for word in words:\n",
    "        tokens.append(word_to_idx.get(word, word_to_idx['<UNK>']))\n",
    "    \n",
    "    if add_special_tokens:\n",
    "        tokens.append(word_to_idx['<EOS>'])\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def detokenize(tokens, idx_to_word):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        words.append(idx_to_word[token])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "644a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"hello world\"\n",
    "# word_to_idx, idx_to_word = create_vocab(sentence)\n",
    "# tokens = tokenize(sentence, word_to_idx)\n",
    "# reconstructed = detokenize(tokens, idx_to_word)\n",
    "# print(f\"Original: {sentence}\")\n",
    "# print(f\"Tokens: {tokens}\")\n",
    "# print(f\"Reconstructed: {reconstructed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22e44616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_tokens = tokenize(self.src_sentences[idx], self.src_vocab)\n",
    "        tgt_tokens = tokenize(self.tgt_sentences[idx], self.tgt_vocab)\n",
    "        return src_tokens, tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4eb91b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     src_tokens, tgt_tokens = zip(*batch)\n",
    "#     tgt_input = tgt_tokens[:-1]\n",
    "#     tgt_output = tgt_tokens[1:]\n",
    "#     padded_src = pad_sequence(torch.tensor(src_tokens))\n",
    "#     padded_tgt_input = pad_sequence(torch.tensor(tgt_input))\n",
    "#     padded_tgt_output = pad_sequence(torch.tensor(tgt_output))\n",
    "    \n",
    "#     return {\n",
    "#         'src': padded_src,\n",
    "#         'tgt_input': padded_tgt_input,\n",
    "#         'tgt_output': padded_tgt_output\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9bf2ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_tokens, tgt_tokens = zip(*batch)\n",
    "    \n",
    "    # Convert each sequence to tensor first\n",
    "    src_tensors = [torch.tensor(seq) for seq in src_tokens]\n",
    "    tgt_tensors = [torch.tensor(seq) for seq in tgt_tokens]\n",
    "    \n",
    "    # Create input and output targets for each sequence\n",
    "    tgt_input_tensors = [seq[:-1] for seq in tgt_tensors]  # Remove last token\n",
    "    tgt_output_tensors = [seq[1:] for seq in tgt_tensors]  # Remove first token\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_src = pad_sequence(src_tensors, batch_first=True, padding_value=0)\n",
    "    padded_tgt_input = pad_sequence(tgt_input_tensors, batch_first=True, padding_value=0)\n",
    "    padded_tgt_output = pad_sequence(tgt_output_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'src': padded_src,\n",
    "        'tgt_input': padded_tgt_input,\n",
    "        'tgt_output': padded_tgt_output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c03ddea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mock data\n",
    "# batch = [\n",
    "#     ([2, 5, 8, 3], [2, 10, 11, 3]),      # src, tgt\n",
    "#     ([2, 12, 7, 9, 15, 3], [2, 20, 21, 22, 3])  # longer sequences\n",
    "# ]\n",
    "\n",
    "# result = collate_fn(batch)\n",
    "# print(\"Shapes:\")\n",
    "# print(f\"src: {result['src'].shape}\")\n",
    "# print(f\"tgt_input: {result['tgt_input'].shape}\")\n",
    "# print(f\"tgt_output: {result['tgt_output'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c8a35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_scheduler(d_model,step_num, warmup_steps):\n",
    "#     l_rate = d_model**(-1/2)*np.min(step_num**(-1/2), step_num*warmup_steps**(-3/2))\n",
    "#     return l_rate\n",
    "\n",
    "class TransformerLRScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "        \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.d_model ** (-0.5) * min(self.step_num ** (-0.5), \n",
    "                                         self.step_num * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50572f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_loss(logits, targets, pad_token_id=0):\n",
    "#     # logits: (batch_size, seq_len, vocab_size)\n",
    "#     # targets: (batch_size, seq_len)\n",
    "    \n",
    "#     # Reshape for loss calculation\n",
    "#     logits = logits.view(-1, logits.size(-1))  # (batch_size * seq_len, vocab_size)\n",
    "#     targets = targets.view(-1)  # (batch_size * seq_len,)\n",
    "    \n",
    "#     # CrossEntropyLoss with ignore_index\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "#     loss = criterion(logits, targets)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1642d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Understanding how compute loss works \n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import numpy as np\n",
    "\n",
    "# # Let's create a concrete example to understand the loss calculation\n",
    "\n",
    "# # Example setup\n",
    "# batch_size = 2\n",
    "# seq_len = 4\n",
    "# vocab_size = 6\n",
    "# pad_token_id = 0\n",
    "\n",
    "# # Example logits from model output\n",
    "# # Shape: (batch_size, seq_len, vocab_size)\n",
    "# logits = torch.randn(batch_size, seq_len, vocab_size)\n",
    "# print(f\"Original logits shape: {logits.shape}\")\n",
    "# print(f\"Logits for first position of first batch:\\n{logits[0, 0, :]}\")\n",
    "\n",
    "# # Example targets (ground truth tokens)\n",
    "# # Shape: (batch_size, seq_len)\n",
    "# targets = torch.tensor([\n",
    "#     [1, 2, 3, 0],  # First sequence: word_ids [1,2,3] + padding [0]\n",
    "#     [4, 5, 0, 0]   # Second sequence: word_ids [4,5] + padding [0,0]\n",
    "# ])\n",
    "# print(f\"\\nOriginal targets shape: {targets.shape}\")\n",
    "# print(f\"Targets:\\n{targets}\")\n",
    "\n",
    "# # Step 1: Reshape for CrossEntropyLoss\n",
    "# # CrossEntropyLoss expects:\n",
    "# # - Input: (N, C) where N = number of samples, C = number of classes\n",
    "# # - Target: (N,) where each value is the class index\n",
    "\n",
    "# logits_reshaped = logits.view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
    "# targets_reshaped = targets.view(-1)            # (batch_size * seq_len,)\n",
    "\n",
    "# print(f\"\\nAfter reshaping:\")\n",
    "# print(f\"Logits shape: {logits_reshaped.shape}\")\n",
    "# print(f\"Targets shape: {targets_reshaped.shape}\")\n",
    "# print(f\"Reshaped targets: {targets_reshaped}\")\n",
    "\n",
    "# # Step 2: Apply CrossEntropyLoss\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "# loss = criterion(logits_reshaped, targets_reshaped)\n",
    "\n",
    "# print(f\"\\nFinal loss: {loss.item()}\")\n",
    "\n",
    "# # Let's manually see what happens with ignore_index\n",
    "# print(f\"\\nWhich positions are ignored (pad_token_id={pad_token_id}):\")\n",
    "# mask = targets_reshaped != pad_token_id\n",
    "# print(f\"Valid positions: {mask}\")\n",
    "# print(f\"Valid targets: {targets_reshaped[mask]}\")\n",
    "\n",
    "# # Manual calculation to show the math\n",
    "# print(f\"\\n--- Manual CrossEntropy Calculation ---\")\n",
    "\n",
    "# # For each valid position, calculate -log(softmax(logits)[target])\n",
    "# valid_losses = []\n",
    "# for i in range(len(targets_reshaped)):\n",
    "#     if targets_reshaped[i] != pad_token_id:  # Skip padding\n",
    "#         # Get logits for this position\n",
    "#         position_logits = logits_reshaped[i]\n",
    "#         true_class = targets_reshaped[i]\n",
    "        \n",
    "#         # Apply softmax\n",
    "#         softmax_probs = F.softmax(position_logits, dim=0)\n",
    "        \n",
    "#         # Cross entropy: -log(probability of true class)\n",
    "#         ce_loss = -torch.log(softmax_probs[true_class])\n",
    "#         valid_losses.append(ce_loss.item())\n",
    "        \n",
    "#         print(f\"Position {i}: target={true_class}, prob={softmax_probs[true_class]:.4f}, loss={ce_loss:.4f}\")\n",
    "\n",
    "# manual_loss = np.mean(valid_losses)\n",
    "# print(f\"\\nManual average loss: {manual_loss:.4f}\")\n",
    "# print(f\"PyTorch loss: {loss.item():.4f}\")\n",
    "\n",
    "# # Show the mathematical formula\n",
    "# print(f\"\\n--- Mathematical Formula ---\")\n",
    "# print(\"CrossEntropyLoss(x, y) = -log(softmax(x)[y])\")\n",
    "# print(\"where:\")\n",
    "# print(\"- x is the logits vector for one sample\")\n",
    "# print(\"- y is the true class index\")\n",
    "# print(\"- softmax(x)[y] is the predicted probability for the true class\")\n",
    "# print(\"\\nFor multiple samples: average over all valid (non-padded) positions\")\n",
    "\n",
    "# # Example with actual word meanings\n",
    "# print(f\"\\n--- Intuitive Example ---\")\n",
    "# vocab = {0: '<PAD>', 1: 'hello', 2: 'world', 3: 'how', 4: 'are', 5: 'you'}\n",
    "# print(\"If our vocabulary is:\", vocab)\n",
    "# print(\"And our target sequence is: [hello, world, how, <PAD>]\")\n",
    "# print(\"The model needs to predict:\")\n",
    "# print(\"- Position 0: 'hello' (class 1)\")\n",
    "# print(\"- Position 1: 'world' (class 2)\")  \n",
    "# print(\"- Position 2: 'how' (class 3)\")\n",
    "# print(\"- Position 3: <PAD> (ignored)\")\n",
    "# print(\"\\nThe loss measures how well the model's probability distribution\")\n",
    "# print(\"matches the true next word at each position (ignoring padding).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3b81bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is AI generated and doesn't make much sense, write in ur own words\n",
    "# Also no need for this, https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html they added label_smoothing to pytorch\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, vocab_size, smoothing=0.1, ignore_index=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.smoothing = smoothing\n",
    "        self.ignore_index = ignore_index\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        # logits: (batch_size, seq_len, vocab_size)\n",
    "        # targets: (batch_size, seq_len) - still hard labels initially\n",
    "        x\n",
    "        batch_size, seq_len = targets.shape\n",
    "        \n",
    "        # Create smoothed labels\n",
    "        smoothed_targets = torch.zeros(batch_size, seq_len, self.vocab_size)\n",
    "        \n",
    "        # Fill with smoothing value\n",
    "        smoothed_targets.fill_(self.smoothing / (self.vocab_size - 1))\n",
    "        \n",
    "        # Set correct class to (1 - smoothing)\n",
    "        smoothed_targets.scatter_(2, targets.unsqueeze(-1), 1.0 - self.smoothing)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        mask = (targets != self.ignore_index).unsqueeze(-1)\n",
    "        smoothed_targets = smoothed_targets * mask\n",
    "        \n",
    "        # Compute KL divergence loss\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.kl_div(log_probs, smoothed_targets, reduction='batchmean')\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e7462f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Extract sentences\n",
    "src_sentences = [item['de'] for item in train_data]\n",
    "tgt_sentences = [item['en'] for item in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a1213c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "word_to_idx_src, idx_to_words_src = create_vocab(src_sentences)\n",
    "word_to_idx_tgt, idx_to_words_tgt = create_vocab(tgt_sentences)\n",
    "\n",
    "# Assuming you have your source and target sentences\n",
    "train_dataset = TranslationDataset(src_sentences, tgt_sentences, word_to_idx_src, word_to_idx_tgt)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2f74470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, dataloader, optimizer, criterion, scheduler=None):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch in dataloader:\n",
    "#         src = batch['src']\n",
    "#         tgt_input = batch['tgt_input'] \n",
    "#         tgt_output = batch['tgt_output']\n",
    "        \n",
    "#         # Forward pass\n",
    "#         logits = model(src, tgt_input)\n",
    "        \n",
    "#         # Loss calculation (this needs special handling!)\n",
    "#         loss = compute_loss(logits=logits, targets=tgt_output)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if scheduler:\n",
    "#             scheduler.step()\n",
    "            \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42736fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() missing 1 required positional argument: 'seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m d_ff = \u001b[32m512\u001b[39m\n\u001b[32m      9\u001b[39m seq_len = \u001b[32m50\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Create dummy data\u001b[39;00m\n\u001b[32m     14\u001b[39m batch_size = \u001b[32m4\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: Transformer.__init__() missing 1 required positional argument: 'seq_len'"
     ]
    }
   ],
   "source": [
    "# Quick test to ensure model doesn't crash\n",
    "def test_model():\n",
    "    # Small test parameters\n",
    "    vocab_size = 1000\n",
    "    d_model = 128\n",
    "    num_heads = 8\n",
    "    num_layers = 2\n",
    "    d_ff = 512\n",
    "    seq_len = 50\n",
    "    \n",
    "    model = Transformer(vocab_size, d_model, num_heads, num_layers, d_ff, seq_len)\n",
    "    \n",
    "    # Create dummy data\n",
    "    batch_size = 4\n",
    "    src_tokens = torch.randint(0, vocab_size, (batch_size, 20))\n",
    "    tgt_tokens = torch.randint(0, vocab_size, (batch_size, 15))\n",
    "    \n",
    "    # Test forward pass\n",
    "    try:\n",
    "        output = model(src_tokens, tgt_tokens)\n",
    "        print(f\"✅ Model works! Output shape: {output.shape}\")\n",
    "        print(f\"Expected shape: (batch_size={batch_size}, seq_len=15, vocab_size={vocab_size})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    criterion = LabelSmoothingLoss(vocab_size=len(word_to_idx_tgt), smoothing=0.1)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        src = batch['src']\n",
    "        tgt_input = batch['tgt_input'] \n",
    "        tgt_output = batch['tgt_output']\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Loss with label smoothing\n",
    "        loss = criterion(logits, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract validation sentences  \n",
    "val_src_sentences = [item['de'] for item in valid_data]\n",
    "val_tgt_sentences = [item['en'] for item in valid_data]\n",
    "\n",
    "# Create validation dataset using the SAME vocabulary as training\n",
    "val_dataset = TranslationDataset(val_src_sentences, val_tgt_sentences, \n",
    "                                word_to_idx_src, word_to_idx_tgt)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "752d0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Data Pipeline ===\n",
      "Batch shapes:\n",
      "  src: torch.Size([32, 22])\n",
      "  tgt_input: torch.Size([32, 20])\n",
      "  tgt_output: torch.Size([32, 20])\n",
      "  model output: torch.Size([32, 20, 15460])\n",
      "✅ Data pipeline works!\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(word_to_idx_src)\n",
    "tgt_vocab_size = len(word_to_idx_tgt)\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "seq_len = 50\n",
    "\n",
    "model = Transformer(src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, d_model=d_model, num_heads=num_heads, num_layers=num_layers, d_ff=d_ff, seq_len=seq_len)\n",
    "\n",
    "def test_data_pipeline():\n",
    "    print(\"=== Testing Data Pipeline ===\")\n",
    "    \n",
    "    # Test one batch\n",
    "    for batch in train_loader:\n",
    "        src = batch['src']\n",
    "        tgt_input = batch['tgt_input'] \n",
    "        tgt_output = batch['tgt_output']\n",
    "        \n",
    "        print(f\"Batch shapes:\")\n",
    "        print(f\"  src: {src.shape}\")\n",
    "        print(f\"  tgt_input: {tgt_input.shape}\")\n",
    "        print(f\"  tgt_output: {tgt_output.shape}\")\n",
    "        \n",
    "        # Test with model\n",
    "        try:\n",
    "            logits = model(src, tgt_input)\n",
    "            print(f\"  model output: {logits.shape}\")\n",
    "            print(\"✅ Data pipeline works!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Data pipeline failed: {e}\")\n",
    "        \n",
    "        break  # Only test first batch\n",
    "\n",
    "# Run this after you fix vocabulary\n",
    "test_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "641e7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Batch 1/3: Loss = 9.6717\n",
      "Batch 2/3: Loss = 9.7118\n",
      "Batch 3/3: Loss = 9.6744\n",
      "✅ Quick training test passed!\n"
     ]
    }
   ],
   "source": [
    "def quick_training_test():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create model with correct vocab size\n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(word_to_idx_src),\n",
    "        tgt_vocab_size=len(word_to_idx_tgt), \n",
    "        d_model=128,\n",
    "        num_heads=8, \n",
    "        num_layers=2,\n",
    "        d_ff=256,\n",
    "        seq_len=50\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Use CrossEntropyLoss with label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "    \n",
    "    # Train on just 3 batches\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i >= 3:\n",
    "            break\n",
    "            \n",
    "        src = batch['src'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss\n",
    "        # logits: (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "        # targets: (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = tgt_output.view(-1)\n",
    "        \n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Batch {i+1}/3: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"✅ Quick training test passed!\")\n",
    "\n",
    "quick_training_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ad03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use CrossEntropyLoss with label smoothing instead of custom loss\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        src = batch['src']\n",
    "        tgt_input = batch['tgt_input'] \n",
    "        tgt_output = batch['tgt_output']\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = tgt_output.view(-1)\n",
    "        \n",
    "        # Loss with label smoothing\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60d74d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean this us later, read it thoroughly too!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64696394",
   "metadata": {},
   "source": [
    "Excellent! Now that everything is working, let's set up proper training with evaluation and monitoring.\n",
    "\n",
    "## Step 1: Set up validation data\n",
    "\n",
    "```python\n",
    "# Create validation dataset\n",
    "val_src_sentences = [item['de'] for item in valid_data]\n",
    "val_tgt_sentences = [item['en'] for item in valid_data]\n",
    "\n",
    "val_dataset = TranslationDataset(val_src_sentences, val_tgt_sentences, \n",
    "                                word_to_idx_src, word_to_idx_tgt)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "```\n",
    "\n",
    "## Step 2: Enhanced evaluation function\n",
    "\n",
    "```python\n",
    "def evaluate_model(model, dataloader, device, idx_to_word_tgt, max_batches=50):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= max_batches:  # Don't evaluate entire dataset\n",
    "                break\n",
    "                \n",
    "            src = batch['src'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_output = batch['tgt_output'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(src, tgt_input)\n",
    "            \n",
    "            # Compute loss\n",
    "            logits_flat = logits.view(-1, logits.size(-1))\n",
    "            targets_flat = tgt_output.view(-1)\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Collect predictions for BLEU score (first 5 batches only)\n",
    "            if i < 5:\n",
    "                pred_ids = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                for j in range(min(2, src.size(0))):  # First 2 samples per batch\n",
    "                    # Convert to words\n",
    "                    pred_tokens = pred_ids[j].cpu().numpy()\n",
    "                    ref_tokens = tgt_output[j].cpu().numpy()\n",
    "                    \n",
    "                    # Convert to words and remove special tokens\n",
    "                    pred_words = []\n",
    "                    ref_words = []\n",
    "                    \n",
    "                    for token in pred_tokens:\n",
    "                        word = idx_to_word_tgt.get(token, '<UNK>')\n",
    "                        if word not in ['<PAD>', '<BOS>', '<EOS>']:\n",
    "                            pred_words.append(word)\n",
    "                    \n",
    "                    for token in ref_tokens:\n",
    "                        word = idx_to_word_tgt.get(token, '<UNK>')\n",
    "                        if word not in ['<PAD>', '<BOS>', '<EOS>']:\n",
    "                            ref_words.append(word)\n",
    "                    \n",
    "                    predictions.append(' '.join(pred_words))\n",
    "                    references.append(' '.join(ref_words))\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    bleu_score = compute_bleu(predictions, references) if predictions else 0\n",
    "    \n",
    "    return avg_loss, bleu_score, predictions[:5], references[:5]\n",
    "\n",
    "def compute_bleu(predictions, references):\n",
    "    \"\"\"Simple BLEU approximation\"\"\"\n",
    "    total_score = 0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_words = set(pred.split())\n",
    "        ref_words = set(ref.split())\n",
    "        \n",
    "        if len(pred_words) == 0:\n",
    "            score = 0\n",
    "        else:\n",
    "            common_words = pred_words & ref_words\n",
    "            precision = len(common_words) / len(pred_words)\n",
    "            recall = len(common_words) / len(ref_words) if len(ref_words) > 0 else 0\n",
    "            score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    return total_score / len(predictions)\n",
    "```\n",
    "\n",
    "## Step 3: Training function with monitoring\n",
    "\n",
    "```python\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, save_path='best_model.pth'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = TransformerLRScheduler(optimizer, d_model=128, warmup_steps=4000)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'bleu_scores': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_bleu = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Training\n",
    "        print(\"Training...\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"Evaluating...\")\n",
    "        val_loss, bleu, pred_samples, ref_samples = evaluate_model(\n",
    "            model, val_loader, device, idx_to_words_tgt\n",
    "        )\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['bleu_scores'].append(bleu)\n",
    "        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"BLEU Score: {bleu:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if bleu > best_bleu:\n",
    "            best_bleu = bleu\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'bleu_score': bleu,\n",
    "            }, save_path)\n",
    "            print(f\"💾 New best model saved! BLEU: {bleu:.4f}\")\n",
    "        \n",
    "        # Show sample translations\n",
    "        print(\"\\nSample translations:\")\n",
    "        for i, (pred, ref) in enumerate(zip(pred_samples[:3], ref_samples[:3])):\n",
    "            print(f\"  {i+1}. Pred: {pred}\")\n",
    "            print(f\"     Ref:  {ref}\")\n",
    "            print()\n",
    "    \n",
    "    return history\n",
    "```\n",
    "\n",
    "## Step 4: Create and start training\n",
    "\n",
    "```python\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(word_to_idx_src),\n",
    "    tgt_vocab_size=len(word_to_idx_tgt),\n",
    "    d_model=256,      # Slightly larger model\n",
    "    num_heads=8,\n",
    "    num_layers=4,     # More layers\n",
    "    d_ff=1024,        # Larger feedforward\n",
    "    seq_len=100       # Longer sequences\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Start training\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=5)\n",
    "```\n",
    "\n",
    "## Step 5: Visualization\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(history['train_loss'], label='Train Loss', color='blue')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss', color='red')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # BLEU score\n",
    "    ax2.plot(history['bleu_scores'], label='BLEU Score', color='green')\n",
    "    ax2.set_title('BLEU Score Over Time')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('BLEU Score')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax3.plot(history['learning_rates'], label='Learning Rate', color='orange')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Step')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    # Loss difference\n",
    "    loss_diff = [abs(t - v) for t, v in zip(history['train_loss'], history['val_loss'])]\n",
    "    ax4.plot(loss_diff, label='|Train - Val| Loss', color='purple')\n",
    "    ax4.set_title('Train-Val Loss Difference')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Loss Difference')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# After training completes:\n",
    "# plot_training_curves(history)\n",
    "```\n",
    "\n",
    "## Your tasks:\n",
    "\n",
    "1. **Set up validation data** - run the validation dataset creation\n",
    "2. **Start with a small test** - maybe 2 epochs first to see if everything works\n",
    "3. **Monitor the training** - watch the loss and BLEU scores\n",
    "\n",
    "**Questions**:\n",
    "1. What's the total number of parameters in your model?\n",
    "2. How long does one epoch take on your machine?\n",
    "3. Are you seeing the train/val loss decreasing?\n",
    "\n",
    "Let me know how the training goes!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd8050",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundational-ml-papers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
