{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBMad5ckz4HBVrWxQdKTeO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goyalpramod/paper_implementations/blob/main/GANs_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code implementation of the following paper -> [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661)"
      ],
      "metadata": {
        "id": "zFJfdDNtNGMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This work has been highly influenced by the following implemnetations\n",
        "\n",
        "* [bamos/dcgan-completion.tensorflow](https://github.com/bamos/dcgan-completion.tensorflow)\n",
        "* [soumith/dcgan.torch](https://github.com/soumith/dcgan.torch/blob/master/main.lua) [The original pytorch implementation, you can find the tf version in alec's github]\n",
        "* [openai/improved-gan](https://github.com/openai/improved-gan)\n",
        "* [lilianweng/unified-gan-tensorflow](https://github.dev/lilianweng/unified-gan-tensorflow) [I picked this one to follow (no reasons)]\n",
        "* [carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow) [Most of the implementation build on this]"
      ],
      "metadata": {
        "id": "zalcj3iZNkOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also check out the following blogs for more clarity on the topic\n",
        "* [Brandon's Blog](https://bamos.github.io/2016/08/09/deep-completion/)\n",
        "* [Lil'log's Blog](https://lilianweng.github.io/posts/2017-08-20-gan/)"
      ],
      "metadata": {
        "id": "R9NfTmncOO4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets understand some equations from the paper\n",
        "\n",
        "$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]$"
      ],
      "metadata": {
        "id": "SOlZwZzuOrqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\min_G \\max_D V(D,G)$\n",
        "\n",
        "This notation means:\n",
        "\n",
        "* The discriminator D tries to maximize V(D,G)\n",
        "* The generator G tries to minimize V(D,G)\n",
        "\n",
        "This creates an adversarial game between G and D"
      ],
      "metadata": {
        "id": "0LV-R9OoOrmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)]$\n",
        "\n",
        "This represents:\n",
        "* Expected value (ð”¼) over samples x drawn from the real data distribution pdata(x)\n",
        "* D(x) outputs a probability between 0 and 1 (how \"real\" D thinks x is)\n",
        "* log D(x) rewards D for correctly identifying real samples\n",
        "* When D(x)â†’1 for real data, log D(x)â†’0, maximizing the objective"
      ],
      "metadata": {
        "id": "7S88JELwQC0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]$\n",
        "\n",
        "This represents:\n",
        "\n",
        "* Expected value over random noise z drawn from noise distribution pz(z)\n",
        "* G(z) generates fake samples from noise\n",
        "* D(G(z)) is the discriminator's output on generated samples\n",
        "* log(1-D(G(z))) rewards D for correctly identifying fake samples\n",
        "* When D(G(z))â†’0 for fake data, log(1-D(G(z)))â†’0, maximizing the objective"
      ],
      "metadata": {
        "id": "uw3n64oYQCxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D^*_G(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$"
      ],
      "metadata": {
        "id": "pGdzJxJ4QCvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "g_s0aCsWQBVl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Any function you do not understand you can add a '?' and run to see its definition\n",
        "# nn.ConvTranspose2d?\n",
        "# What will happen if we do 'nn.ConvTranspose2d??', run and find out!!"
      ],
      "metadata": {
        "id": "xmsOFEfWc7qx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: (batch_size, latent_dim)  # typically latent_dim = 100\n",
        "    Output: (batch_size, channels, height, width)  # e.g., (batch_size, 3, 64, 64)\n",
        "\n",
        "    Architecture Hints:\n",
        "    1. Start with Dense layer to reshape latent vector\n",
        "    2. Use multiple ConvTranspose2d layers to upscale\n",
        "    3. Each layer should follow pattern: ConvTranspose2d -> BatchNorm2d -> ReLU\n",
        "    4. Final layer should use Tanh activation\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Step 1: Dense layer to reshape\n",
        "        # latent_dim -> (4*4*512) features\n",
        "        # Hint: Use Linear layer here\n",
        "\n",
        "        # Step 2: Reshape to (batch_size, 512, 4, 4)\n",
        "\n",
        "        # Step 3: Multiple ConvTranspose2d blocks\n",
        "        # Block 1: (512, 4, 4) -> (256, 8, 8)\n",
        "        # kernel_size=4, stride=2, padding=1\n",
        "\n",
        "        # Block 2: (256, 8, 8) -> (128, 16, 16)\n",
        "        # Same parameters as above\n",
        "\n",
        "        # Block 3: (128, 16, 16) -> (64, 32, 32)\n",
        "\n",
        "        # Final Block: (64, 32, 32) -> (3, 64, 64)\n",
        "        # Don't forget Tanh() at the end!"
      ],
      "metadata": {
        "id": "wokUPP0MaIUC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Dense layer\n",
        "        self.linear = nn.Linear(latent_dim, 4*4*512)\n",
        "\n",
        "        # Blocks\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Changed input channels from 3 to 1 for MNIST\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, latent_dim)\n",
        "        x = self.linear(x)\n",
        "        x = x.view(-1, 512, 4, 4)  # reshape using view\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RA5foquLfJGN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input: (batch_size, channels, height, width)  # e.g., (batch_size, 3, 64, 64)\n",
        "    Output: (batch_size, 1)  # probability between 0-1\n",
        "\n",
        "    Architecture Hints:\n",
        "    1. Use Conv2d layers to downsample\n",
        "    2. Each layer: Conv2d -> BatchNorm2d (except first) -> LeakyReLU\n",
        "    3. Final layer should be Dense with Sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Block 1: (3, 64, 64) -> (64, 32, 32)\n",
        "        # Conv2d: kernel_size=4, stride=2, padding=1\n",
        "        # Don't use BatchNorm in first layer!\n",
        "\n",
        "        # Block 2: (64, 32, 32) -> (128, 16, 16)\n",
        "        # Remember BatchNorm here\n",
        "\n",
        "        # Block 3: (128, 16, 16) -> (256, 8, 8)\n",
        "\n",
        "        # Block 4: (256, 8, 8) -> (512, 4, 4)\n",
        "\n",
        "        # Final: Flatten and Dense to single output\n",
        "        # Don't forget Sigmoid()!"
      ],
      "metadata": {
        "id": "eewhlwj4aKyF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Changed input channels from 3 to 1 for MNIST\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(512*4*4, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.view(-1, 512*4*4)  # flatten\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Nuj0oOtxj-BE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(real_images, generator, discriminator, criterion,\n",
        "               optimizer_G, optimizer_D, latent_dim, device):\n",
        "    batch_size = real_images.size(0)\n",
        "\n",
        "    # 1. Train Discriminator\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # 1a. Train on real batch\n",
        "    # Hint: Create labels for real images (all ones)\n",
        "    labels_real = torch.ones(batch_size, 1).to(device)\n",
        "    # Hint: Forward real images through D\n",
        "    d_output_real = discriminator(real_images)\n",
        "    # Hint: Calculate loss for real images\n",
        "    loss_real = criterion(d_output_real, labels_real)\n",
        "\n",
        "    # 1b. Train on fake batch\n",
        "    # Hint: Generate noise z\n",
        "    z = torch.randn(batch_size, latent_dim).to(device)\n",
        "    # Hint: Generate fake images using G(z)\n",
        "    g_output = generator(z)\n",
        "    # Hint: Create labels for fake images (all zeros)\n",
        "    labels_fake = torch.zeros(batch_size,1).to(device)\n",
        "    # Hint: Forward fake images through D\n",
        "    d_output_fake = discriminator(g_output.detach())\n",
        "    # Why: Get discriminator's prediction on fake images\n",
        "    # Note: detach() prevents gradients flowing to generator\n",
        "    # Doc: tensor.detach() - creates new tensor that shares storage but no gradients\n",
        "    # Hint: Calculate loss for fake images\n",
        "    loss_fake = criterion(d_output_fake,labels_fake)\n",
        "\n",
        "    # 1c. Add both losses and update D\n",
        "    # Hint: d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss = loss_real + loss_fake\n",
        "    # Hint: backward and step\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "\n",
        "\n",
        "    # 1. Clear generator gradients\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # 2. Generate NEW fake images\n",
        "    # Note: We can reuse the same z from before\n",
        "    fake_images = generator(z)\n",
        "\n",
        "    # 3. Get discriminator's prediction on fake images\n",
        "    # Note: This time we DON'T use detach() because we want to train G\n",
        "    d_output_fake = discriminator(fake_images)\n",
        "\n",
        "    # 4. Calculate generator loss\n",
        "    # Important: We use REAL labels (ones) here, not zeros!\n",
        "    # Why? We want the generator to trick discriminator into thinking fakes are real\n",
        "    g_loss = criterion(d_output_fake, labels_real)  # try to make D think these are real!\n",
        "\n",
        "    # 5. Backpropagate and update\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    return d_loss.item(), g_loss.item()"
      ],
      "metadata": {
        "id": "bERsOhGMaNSp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),  # Resize images to 64x64\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                   train=True,\n",
        "                                   transform=transform,\n",
        "                                   download=True)\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64,\n",
        "                                       shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "3dJZ6-kmaNgS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "latent_dim = 100\n",
        "num_epochs = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# Initialize optimizers\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "xDlJ7-31aNn4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store losses for plotting\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(dataloader):\n",
        "        # Configure input\n",
        "        real_images = real_images.to(device)\n",
        "\n",
        "        # Train models\n",
        "        d_loss, g_loss = train_step(real_images, generator, discriminator,\n",
        "                                  criterion, optimizer_G, optimizer_D, latent_dim, device)\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(g_loss)\n",
        "        D_losses.append(d_loss)\n",
        "\n",
        "        # Print progress\n",
        "        if i % 100 == 0:\n",
        "            print(f'[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] '\n",
        "                  f'[D loss: {d_loss:.4f}] [G loss: {g_loss:.4f}]')\n",
        "\n",
        "    # Generate and save sample images after each epoch\n",
        "    if epoch % 5 == 0:\n",
        "        with torch.no_grad():\n",
        "            fake = generator(torch.randn(16, latent_dim, device=device))\n",
        "            # Save the images (we'll implement this next)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhu50QN9HTdh",
        "outputId": "431adf61-47b7-446c-813a-2562898971ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "[Epoch 0/100] [Batch 0/938] [D loss: 1.4458] [G loss: 2.0241]\n",
            "[Epoch 0/100] [Batch 100/938] [D loss: 0.0007] [G loss: 8.0421]\n",
            "[Epoch 0/100] [Batch 200/938] [D loss: 0.0003] [G loss: 9.0054]\n",
            "[Epoch 0/100] [Batch 300/938] [D loss: 100.0000] [G loss: 0.0000]\n",
            "[Epoch 0/100] [Batch 400/938] [D loss: 0.7979] [G loss: 1.9469]\n",
            "[Epoch 0/100] [Batch 500/938] [D loss: 1.3899] [G loss: 1.8981]\n",
            "[Epoch 0/100] [Batch 600/938] [D loss: 0.5952] [G loss: 4.0113]\n",
            "[Epoch 0/100] [Batch 700/938] [D loss: 0.3971] [G loss: 2.7752]\n",
            "[Epoch 0/100] [Batch 800/938] [D loss: 0.7386] [G loss: 1.3431]\n",
            "[Epoch 0/100] [Batch 900/938] [D loss: 0.4414] [G loss: 1.9620]\n",
            "[Epoch 1/100] [Batch 0/938] [D loss: 0.9388] [G loss: 1.0397]\n",
            "[Epoch 1/100] [Batch 100/938] [D loss: 0.7561] [G loss: 2.5330]\n",
            "[Epoch 1/100] [Batch 200/938] [D loss: 0.1322] [G loss: 6.2223]\n",
            "[Epoch 1/100] [Batch 300/938] [D loss: 0.3843] [G loss: 1.3346]\n",
            "[Epoch 1/100] [Batch 400/938] [D loss: 0.2888] [G loss: 2.3259]\n",
            "[Epoch 1/100] [Batch 500/938] [D loss: 0.1194] [G loss: 3.5779]\n",
            "[Epoch 1/100] [Batch 600/938] [D loss: 0.3466] [G loss: 2.0517]\n",
            "[Epoch 1/100] [Batch 700/938] [D loss: 0.0389] [G loss: 5.0075]\n",
            "[Epoch 1/100] [Batch 800/938] [D loss: 0.9880] [G loss: 6.2351]\n",
            "[Epoch 1/100] [Batch 900/938] [D loss: 0.0244] [G loss: 5.4358]\n",
            "[Epoch 2/100] [Batch 0/938] [D loss: 0.0487] [G loss: 4.0305]\n",
            "[Epoch 2/100] [Batch 100/938] [D loss: 0.6159] [G loss: 2.8915]\n",
            "[Epoch 2/100] [Batch 200/938] [D loss: 0.5314] [G loss: 3.7871]\n",
            "[Epoch 2/100] [Batch 300/938] [D loss: 0.5118] [G loss: 4.1742]\n",
            "[Epoch 2/100] [Batch 400/938] [D loss: 0.2108] [G loss: 3.2547]\n",
            "[Epoch 2/100] [Batch 500/938] [D loss: 0.2489] [G loss: 2.7105]\n",
            "[Epoch 2/100] [Batch 600/938] [D loss: 0.0251] [G loss: 5.8204]\n",
            "[Epoch 2/100] [Batch 700/938] [D loss: 0.3745] [G loss: 4.0879]\n",
            "[Epoch 2/100] [Batch 800/938] [D loss: 0.3517] [G loss: 9.0886]\n",
            "[Epoch 2/100] [Batch 900/938] [D loss: 0.0149] [G loss: 6.1066]\n",
            "[Epoch 3/100] [Batch 0/938] [D loss: 0.0134] [G loss: 5.0078]\n",
            "[Epoch 3/100] [Batch 100/938] [D loss: 0.9580] [G loss: 1.5227]\n",
            "[Epoch 3/100] [Batch 200/938] [D loss: 0.9044] [G loss: 2.4596]\n",
            "[Epoch 3/100] [Batch 300/938] [D loss: 0.0152] [G loss: 6.0000]\n",
            "[Epoch 3/100] [Batch 400/938] [D loss: 0.0174] [G loss: 5.3135]\n",
            "[Epoch 3/100] [Batch 500/938] [D loss: 0.0848] [G loss: 4.0407]\n",
            "[Epoch 3/100] [Batch 600/938] [D loss: 0.5234] [G loss: 1.2625]\n",
            "[Epoch 3/100] [Batch 700/938] [D loss: 0.0280] [G loss: 5.6857]\n",
            "[Epoch 3/100] [Batch 800/938] [D loss: 0.0373] [G loss: 5.2212]\n",
            "[Epoch 3/100] [Batch 900/938] [D loss: 0.0330] [G loss: 4.4330]\n",
            "[Epoch 4/100] [Batch 0/938] [D loss: 0.0680] [G loss: 5.5135]\n",
            "[Epoch 4/100] [Batch 100/938] [D loss: 0.0087] [G loss: 5.6995]\n",
            "[Epoch 4/100] [Batch 200/938] [D loss: 0.0076] [G loss: 7.0796]\n",
            "[Epoch 4/100] [Batch 300/938] [D loss: 1.0194] [G loss: 5.2721]\n",
            "[Epoch 4/100] [Batch 400/938] [D loss: 0.4921] [G loss: 6.1053]\n",
            "[Epoch 4/100] [Batch 500/938] [D loss: 0.1776] [G loss: 5.1908]\n",
            "[Epoch 4/100] [Batch 600/938] [D loss: 0.6807] [G loss: 4.2370]\n",
            "[Epoch 4/100] [Batch 700/938] [D loss: 0.0749] [G loss: 6.1340]\n",
            "[Epoch 4/100] [Batch 800/938] [D loss: 0.0615] [G loss: 4.4297]\n",
            "[Epoch 4/100] [Batch 900/938] [D loss: 0.0164] [G loss: 8.9764]\n",
            "[Epoch 5/100] [Batch 0/938] [D loss: 0.0056] [G loss: 8.6457]\n",
            "[Epoch 5/100] [Batch 100/938] [D loss: 0.0108] [G loss: 8.4483]\n",
            "[Epoch 5/100] [Batch 200/938] [D loss: 0.0028] [G loss: 8.0723]\n",
            "[Epoch 5/100] [Batch 300/938] [D loss: 0.0002] [G loss: 10.6186]\n",
            "[Epoch 5/100] [Batch 400/938] [D loss: 0.0009] [G loss: 8.0660]\n",
            "[Epoch 5/100] [Batch 500/938] [D loss: 0.0005] [G loss: 8.1849]\n",
            "[Epoch 5/100] [Batch 600/938] [D loss: 0.0004] [G loss: 8.2756]\n",
            "[Epoch 5/100] [Batch 700/938] [D loss: 0.0006] [G loss: 8.2838]\n",
            "[Epoch 5/100] [Batch 800/938] [D loss: 0.0009] [G loss: 10.1918]\n",
            "[Epoch 5/100] [Batch 900/938] [D loss: 0.1188] [G loss: 13.5286]\n",
            "[Epoch 6/100] [Batch 0/938] [D loss: 0.0002] [G loss: 42.9552]\n",
            "[Epoch 6/100] [Batch 100/938] [D loss: 0.0000] [G loss: 42.3447]\n",
            "[Epoch 6/100] [Batch 200/938] [D loss: 0.0000] [G loss: 42.4397]\n",
            "[Epoch 6/100] [Batch 300/938] [D loss: 0.0000] [G loss: 42.4623]\n",
            "[Epoch 6/100] [Batch 400/938] [D loss: 0.0000] [G loss: 42.2532]\n",
            "[Epoch 6/100] [Batch 500/938] [D loss: 0.0000] [G loss: 42.3103]\n",
            "[Epoch 6/100] [Batch 600/938] [D loss: 0.0000] [G loss: 42.1013]\n",
            "[Epoch 6/100] [Batch 700/938] [D loss: 0.0000] [G loss: 42.1775]\n",
            "[Epoch 6/100] [Batch 800/938] [D loss: 0.0000] [G loss: 41.9808]\n",
            "[Epoch 6/100] [Batch 900/938] [D loss: 0.0000] [G loss: 41.9088]\n",
            "[Epoch 7/100] [Batch 0/938] [D loss: 0.0000] [G loss: 41.8911]\n",
            "[Epoch 7/100] [Batch 100/938] [D loss: 0.0000] [G loss: 41.9526]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_generator_output(fake_images, epoch):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 4, i+1)\n",
        "        plt.imshow(fake_images[i][0].cpu().detach().numpy(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.savefig(f'fake_images_epoch_{epoch}.png')\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "49P_YFnpHTub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOos6dpqHT9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}