{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7X6sFV3B8pwIlGRC7srx3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goyalpramod/paper_implementations/blob/main/UNET_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing UNET from scratch. Read the original [paper here](https://arxiv.org/pdf/1505.04597)\n",
        "\n",
        "This is mostly a scrappy quickly written code with validation from AI, I haven't run it yet.\n",
        "\n",
        "I wanted it to be more guided approach, but I will be adding this in my diffusion model blog, so I will breakdown the ideas and components better there."
      ],
      "metadata": {
        "id": "5aUN-Igonlli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR67LBR3hcHI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hints for DoubleConv class\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement:\n",
        "    1. Two 3x3 convolutions each followed by\n",
        "    2. Batch normalization (optional but recommended)\n",
        "    3. ReLU activation\n",
        "    Use nn.Sequential for clean implementation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "vSgvCMX_n0H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)"
      ],
      "metadata": {
        "id": "PVfoDD6pvZiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down:\n",
        "    \"\"\"\n",
        "    Implement:\n",
        "    1. Max pooling operation (2x2 window)\n",
        "    2. Double convolution\n",
        "    Track the spatial dimension changes!\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "KFL8I2y2n00o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)"
      ],
      "metadata": {
        "id": "2Qk_WpJjv433"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up:\n",
        "    \"\"\"\n",
        "    Implement:\n",
        "    1. Upsampling (either ConvTranspose2d or Upsample)\n",
        "    2. Concatenation with skip connection\n",
        "    3. Double convolution\n",
        "    Remember to handle different input sizes!\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "ngKLr4uyn0-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        # Handle size differences\n",
        "        diff_y = x2.size()[2] - x1.size()[2]\n",
        "        diff_x = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,\n",
        "                       diff_y // 2, diff_y - diff_y // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "Gyd58poSwPL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet:\n",
        "    \"\"\"\n",
        "    Implement:\n",
        "    1. Initial double convolution\n",
        "    2. Downsampling path (typically 4 down steps)\n",
        "    3. Bottleneck\n",
        "    4. Upsampling path with skip connections\n",
        "    5. Final 1x1 convolution to map to desired number of classes\n",
        "\n",
        "    Key points to consider:\n",
        "    - Feature map sizes at each level\n",
        "    - Channel numbers doubling/halving\n",
        "    - Skip connections management\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "N7azpNhzn1Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Current issues:\n",
        "        # 1. Inconsistent channel progression\n",
        "        # 2. Redundant DoubleConv layers\n",
        "        # 3. Bottleneck implementation is incomplete\n",
        "        # 4. Skip connection handling is missing\n",
        "\n",
        "        # Correct channel progression:\n",
        "        self.inc = DoubleConv(3, 64)  # Initial convolution\n",
        "\n",
        "        # Encoder path (feature maps halve, channels double)\n",
        "        self.down1 = Down(64, 128)    # Output: 128 channels\n",
        "        self.down2 = Down(128, 256)   # Output: 256 channels\n",
        "        self.down3 = Down(256, 512)   # Output: 512 channels\n",
        "        self.down4 = Down(512, 1024)  # Output: 1024 channels\n",
        "\n",
        "        # Decoder path (feature maps double, channels halve)\n",
        "        self.up1 = Up(1024, 512)      # Input: 1024 + 512 = 1536 channels\n",
        "        self.up2 = Up(512, 256)       # Input: 512 + 256 = 768 channels\n",
        "        self.up3 = Up(256, 128)       # Input: 256 + 128 = 384 channels\n",
        "        self.up4 = Up(128, 64)        # Input: 128 + 64 = 192 channels\n",
        "\n",
        "        # Final convolution\n",
        "        self.outc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store encoder outputs for skip connections\n",
        "        x1 = self.inc(x)         # [B, 64, H, W]\n",
        "        x2 = self.down1(x1)      # [B, 128, H/2, W/2]\n",
        "        x3 = self.down2(x2)      # [B, 256, H/4, W/4]\n",
        "        x4 = self.down3(x3)      # [B, 512, H/8, W/8]\n",
        "        x5 = self.down4(x4)      # [B, 1024, H/16, W/16]\n",
        "\n",
        "        # Decoder path with skip connections\n",
        "        x = self.up1(x5, x4)     # Use skip connection from x4\n",
        "        x = self.up2(x, x3)      # Use skip connection from x3\n",
        "        x = self.up3(x, x2)      # Use skip connection from x2\n",
        "        x = self.up4(x, x1)      # Use skip connection from x1\n",
        "\n",
        "        # Final 1x1 convolution\n",
        "        logits = self.outc(x)    # [B, num_classes, H, W]\n",
        "\n",
        "        return logits\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "QQ2tmVFxn1IK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [B, 3, H, W]\n",
        "# Encoder:\n",
        "# - Level 1: [B, 64, H, W]\n",
        "# - Level 2: [B, 128, H/2, W/2]\n",
        "# - Level 3: [B, 256, H/4, W/4]\n",
        "# - Level 4: [B, 512, H/8, W/8]\n",
        "# - Bottleneck: [B, 1024, H/16, W/16]\n",
        "\n",
        "# Decoder (with skip connections):\n",
        "# - Level 4: [B, 512, H/8, W/8]\n",
        "# - Level 3: [B, 256, H/4, W/4]\n",
        "# - Level 2: [B, 128, H/2, W/2]\n",
        "# - Level 1: [B, 64, H, W]\n",
        "# Output: [B, n_classes, H, W]"
      ],
      "metadata": {
        "id": "LvqolmKFn1MR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetTrainer:\n",
        "    def __init__(self, model, device, n_classes):\n",
        "        \"\"\"\n",
        "        Initialize trainer with essential components\n",
        "\n",
        "        Args:\n",
        "            model: UNet model\n",
        "            device: torch.device\n",
        "            n_classes: number of segmentation classes\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # 1. Loss Function\n",
        "        # For binary segmentation:\n",
        "        self.criterion = nn.BCEWithLogitsLoss() if n_classes == 2 else \\\n",
        "                        nn.CrossEntropyLoss()\n",
        "\n",
        "        # 2. Optimizer (Adam with learning rate scheduling)\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, 'min', patience=3)\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"\n",
        "        Single training epoch\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for training data\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Move data to device\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            # Zero gradients\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = self.model(data)\n",
        "\n",
        "            # Compute loss\n",
        "            if self.n_classes == 2:\n",
        "                # Binary segmentation\n",
        "                loss = self.criterion(output, target.float())\n",
        "            else:\n",
        "                # Multi-class segmentation\n",
        "                loss = self.criterion(output, target.long())\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        return epoch_loss / len(train_loader)\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        \"\"\"\n",
        "        Validation step\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0\n",
        "        dice_score = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in val_loader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "\n",
        "                # Compute validation metrics\n",
        "                val_loss += self.criterion(output,\n",
        "                    target.float() if self.n_classes == 2 else target.long()).item()\n",
        "\n",
        "                # Compute Dice score\n",
        "                dice_score += self.compute_dice(output, target)\n",
        "\n",
        "        return val_loss / len(val_loader), dice_score / len(val_loader)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_dice(pred, target):\n",
        "        \"\"\"\n",
        "        Compute Dice coefficient\n",
        "\n",
        "        Args:\n",
        "            pred: model predictions\n",
        "            target: ground truth masks\n",
        "        \"\"\"\n",
        "        smooth = 1e-5\n",
        "        pred = torch.sigmoid(pred) if pred.shape[1] == 1 else \\\n",
        "               F.softmax(pred, dim=1)\n",
        "\n",
        "        intersection = (pred * target).sum()\n",
        "        return (2. * intersection + smooth) / \\\n",
        "               (pred.sum() + target.sum() + smooth)"
      ],
      "metadata": {
        "id": "7CrCPhPT87In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100):\n",
        "    trainer = UNetTrainer(model, device='cuda', n_classes=YOUR_N_CLASSES)\n",
        "\n",
        "    best_val_score = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        train_loss = trainer.train_epoch(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, dice_score = trainer.validate(val_loader)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        trainer.scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_score:\n",
        "            best_val_score = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Training Loss: {train_loss:.4f}')\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "        print(f'Dice Score: {dice_score:.4f}')"
      ],
      "metadata": {
        "id": "GKYnzRe-9bj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforms = A.Compose([\n",
        "#     A.RandomRotate90(),\n",
        "#     A.Flip(),\n",
        "#     A.ElasticTransform(),\n",
        "#     A.GridDistortion(),\n",
        "#     A.Normalize()\n",
        "# ])"
      ],
      "metadata": {
        "id": "SJwh6C6R9cju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_metrics(pred, target):\n",
        "#     \"\"\"\n",
        "#     Compute multiple segmentation metrics:\n",
        "#     - IoU (Intersection over Union)\n",
        "#     - Dice Coefficient\n",
        "#     - Pixel Accuracy\n",
        "#     \"\"\"\n",
        "#     pass"
      ],
      "metadata": {
        "id": "AHf_pgCF9eQj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}